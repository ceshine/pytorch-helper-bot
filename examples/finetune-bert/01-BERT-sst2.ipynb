{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from pathlib import Path\n",
    "\n",
    "import nlp\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_AVAILABLE = True\n",
    "except ModuleNotFoundError:\n",
    "    APEX_AVAILABLE = False\n",
    "\n",
    "from pytorch_helper_bot import (\n",
    "    BaseBot, MovingAverageStatsTrackerCallback,  CheckpointCallback,\n",
    "    LearningRateSchedulerCallback, MultiStageScheduler, Top1Accuracy,\n",
    "    LinearLR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(\"cache/\")\n",
    "CACHE_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "    * https://github.com/huggingface/nlp/blob/master/notebooks/Overview.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nlp.load_dataset('glue', \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x['label'] for x in dataset[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our training dataset\n",
    "def convert_to_features(example_batch):\n",
    "    # Tokenize contexts and questions (as pairs of inputs)\n",
    "    encodings = tokenizer.batch_encode_plus(example_batch['sentence'], pad_to_max_length=True, max_length=64)\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format our dataset to outputs torch.Tensor to train a pytorch model\n",
    "columns = ['input_ids', 'token_type_ids', 'attention_mask', \"label\"]\n",
    "for subset in (\"train\", \"validation\"): \n",
    "    dataset[subset] = dataset[subset].map(convert_to_features, batched=True)\n",
    "    dataset[subset].set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small, personal film with an emotional wallop. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset['train'][6][\"input_ids\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0][\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, entries_dict):\n",
    "        super().__init__()\n",
    "        self.entries_dict = entries_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.entries_dict[\"label\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.entries_dict[\"input_ids\"][idx],\n",
    "            self.entries_dict[\"attention_mask\"][idx],\n",
    "            self.entries_dict[\"token_type_ids\"][idx],\n",
    "            self.entries_dict[\"label\"][idx]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx, test_idx = train_test_split(list(range(len(dataset[\"validation\"]))), test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {\n",
    "    \"input_ids\": dataset['train'][\"input_ids\"],\n",
    "    \"attention_mask\": dataset['train'][\"attention_mask\"],\n",
    "    \"token_type_ids\": dataset['train'][\"token_type_ids\"],\n",
    "    \"label\": dataset['train'][\"label\"]\n",
    "}\n",
    "valid_dict = {\n",
    "    \"input_ids\": dataset['validation'][\"input_ids\"][valid_idx],\n",
    "    \"attention_mask\": dataset['validation'][\"attention_mask\"][valid_idx],\n",
    "    \"token_type_ids\": dataset['validation'][\"token_type_ids\"][valid_idx],\n",
    "    \"label\": dataset['validation'][\"label\"][valid_idx]\n",
    "}\n",
    "test_dict = {\n",
    "    \"input_ids\": dataset['validation'][\"input_ids\"][test_idx],\n",
    "    \"attention_mask\": dataset['validation'][\"attention_mask\"][test_idx],\n",
    "    \"token_type_ids\": dataset['validation'][\"token_type_ids\"][test_idx],\n",
    "    \"label\": dataset['validation'][\"label\"][test_idx]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PyTorch Dataloader around our dataset\n",
    "train_loader = torch.utils.data.DataLoader(SST2Dataset(train_dict), batch_size=32, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(SST2Dataset(valid_dict), batch_size=32, drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(SST2Dataset(test_dict), batch_size=32, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class SST2Bot(BaseBot):\n",
    "    log_dir = CACHE_DIR / \"logs\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        self.loss_format = \"%.6f\"\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_prediction(output):\n",
    "        return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.init.kaiming_normal_(model.classifier.weight)\n",
    "# torch.nn.init.constant_(model.classifier.bias, 0)\n",
    "# torch.nn.init.kaiming_normal_(model.bert.pooler.dense.weight)\n",
    "# torch.nn.init.constant_(model.bert.pooler.dense.bias, 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n"
     ]
    }
   ],
   "source": [
    "if APEX_AVAILABLE:\n",
    "    model, optimizer = amp.initialize(\n",
    "        model, optimizer, opt_level=\"O1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][06/15/2020 14:19:31] SEED: 9293\n",
      "[INFO][06/15/2020 14:19:31] # of parameters: 109,483,778\n",
      "[INFO][06/15/2020 14:19:31] # of trainable parameters: 109,483,778\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(train_loader) * 3\n",
    "\n",
    "checkpoints = CheckpointCallback(\n",
    "    keep_n_checkpoints=1,\n",
    "    checkpoint_dir=CACHE_DIR / \"model_cache/\",\n",
    "    monitor_metric=\"accuracy\"\n",
    ")\n",
    "lr_durations = [\n",
    "    int(total_steps*0.2),\n",
    "    int(np.ceil(total_steps*0.8))\n",
    "]\n",
    "break_points = [0] + list(np.cumsum(lr_durations))[:-1]\n",
    "callbacks = [\n",
    "    MovingAverageStatsTrackerCallback(\n",
    "        avg_window=len(train_loader) // 8,\n",
    "        log_interval=len(train_loader) // 10\n",
    "    ),\n",
    "    LearningRateSchedulerCallback(\n",
    "        MultiStageScheduler(\n",
    "            [\n",
    "                LinearLR(optimizer, 0.01, lr_durations[0]),\n",
    "                CosineAnnealingLR(optimizer, lr_durations[1])\n",
    "            ],\n",
    "            start_at_epochs=break_points\n",
    "        )\n",
    "    ),\n",
    "    checkpoints\n",
    "]\n",
    "    \n",
    "bot = SST2Bot(\n",
    "    model=model, \n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader, \n",
    "    clip_grad=10.,\n",
    "    optimizer=optimizer, echo=True,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    callbacks=callbacks,\n",
    "    pbar=False, use_tensorboard=False,\n",
    "    use_amp=APEX_AVAILABLE,\n",
    "    metrics=(Top1Accuracy(),)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][06/15/2020 14:19:31] Optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 2e-05\n",
      "    lr: 2e-05\n",
      "    weight_decay: 0\n",
      ")\n",
      "[INFO][06/15/2020 14:19:31] Batches per epoch: 2105\n",
      "[INFO][06/15/2020 14:19:31] ====================Epoch 1====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6315\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][06/15/2020 14:19:54] Step   210 | loss 0.642613 | lr: 3.49e-06 | 0.113s per step\n",
      "[INFO][06/15/2020 14:20:18] Step   420 | loss 0.363807 | lr: 6.79e-06 | 0.114s per step\n",
      "[INFO][06/15/2020 14:20:42] Step   630 | loss 0.272996 | lr: 1.01e-05 | 0.112s per step\n",
      "[INFO][06/15/2020 14:21:06] Step   840 | loss 0.244767 | lr: 1.34e-05 | 0.117s per step\n",
      "[INFO][06/15/2020 14:21:31] Step  1050 | loss 0.219585 | lr: 1.67e-05 | 0.119s per step\n",
      "[INFO][06/15/2020 14:21:32] Metrics at step 1052:\n",
      "[INFO][06/15/2020 14:21:32] loss: 0.270236\n",
      "[INFO][06/15/2020 14:21:32] accuracy: 91.06%\n",
      "[INFO][06/15/2020 14:21:56] Step  1260 | loss 0.206357 | lr: 2.00e-05 | 0.118s per step\n",
      "[INFO][06/15/2020 14:22:20] Step  1470 | loss 0.194531 | lr: 1.99e-05 | 0.111s per step\n",
      "[INFO][06/15/2020 14:22:43] Step  1680 | loss 0.190104 | lr: 1.97e-05 | 0.111s per step\n",
      "[INFO][06/15/2020 14:23:06] Step  1890 | loss 0.175202 | lr: 1.93e-05 | 0.110s per step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO][06/15/2020 14:23:29] Step  2100 | loss 0.170110 | lr: 1.87e-05 | 0.110s per step\n",
      "[INFO][06/15/2020 14:23:30] Metrics at step 2104:\n",
      "[INFO][06/15/2020 14:23:30] loss: 0.254708\n",
      "[INFO][06/15/2020 14:23:30] accuracy: 91.74%\n",
      "[INFO][06/15/2020 14:23:31] ====================Epoch 2====================\n",
      "[INFO][06/15/2020 14:23:55] Step  2310 | loss 0.132254 | lr: 1.80e-05 | 0.123s per step\n",
      "[INFO][06/15/2020 14:24:19] Step  2520 | loss 0.119210 | lr: 1.71e-05 | 0.116s per step\n",
      "[INFO][06/15/2020 14:24:43] Step  2730 | loss 0.118474 | lr: 1.61e-05 | 0.115s per step\n",
      "[INFO][06/15/2020 14:25:08] Step  2940 | loss 0.127645 | lr: 1.51e-05 | 0.116s per step\n",
      "[INFO][06/15/2020 14:25:31] Step  3150 | loss 0.121345 | lr: 1.39e-05 | 0.111s per step\n",
      "[INFO][06/15/2020 14:25:32] Metrics at step 3156:\n",
      "[INFO][06/15/2020 14:25:32] loss: 0.253494\n",
      "[INFO][06/15/2020 14:25:32] accuracy: 91.06%\n",
      "[INFO][06/15/2020 14:25:54] Step  3360 | loss 0.116062 | lr: 1.27e-05 | 0.112s per step\n",
      "[INFO][06/15/2020 14:26:18] Step  3570 | loss 0.114419 | lr: 1.14e-05 | 0.114s per step\n",
      "[INFO][06/15/2020 14:26:43] Step  3780 | loss 0.121249 | lr: 1.01e-05 | 0.115s per step\n",
      "[INFO][06/15/2020 14:27:07] Step  3990 | loss 0.115698 | lr: 8.77e-06 | 0.114s per step\n",
      "[INFO][06/15/2020 14:27:31] Step  4200 | loss 0.110179 | lr: 7.49e-06 | 0.118s per step\n",
      "[INFO][06/15/2020 14:27:33] Metrics at step 4208:\n",
      "[INFO][06/15/2020 14:27:33] loss: 0.237897\n",
      "[INFO][06/15/2020 14:27:33] accuracy: 91.51%\n",
      "[INFO][06/15/2020 14:27:33] ====================Epoch 3====================\n",
      "[INFO][06/15/2020 14:27:56] Step  4410 | loss 0.069523 | lr: 6.25e-06 | 0.118s per step\n",
      "[INFO][06/15/2020 14:28:21] Step  4620 | loss 0.059564 | lr: 5.07e-06 | 0.119s per step\n",
      "[INFO][06/15/2020 14:28:46] Step  4830 | loss 0.064512 | lr: 3.98e-06 | 0.118s per step\n",
      "[INFO][06/15/2020 14:29:10] Step  5040 | loss 0.057509 | lr: 2.99e-06 | 0.115s per step\n",
      "[INFO][06/15/2020 14:29:35] Step  5250 | loss 0.065172 | lr: 2.12e-06 | 0.118s per step\n",
      "[INFO][06/15/2020 14:29:36] Metrics at step 5260:\n",
      "[INFO][06/15/2020 14:29:36] loss: 0.288512\n",
      "[INFO][06/15/2020 14:29:36] accuracy: 92.20%\n",
      "[INFO][06/15/2020 14:30:00] Step  5460 | loss 0.059740 | lr: 1.39e-06 | 0.120s per step\n",
      "[INFO][06/15/2020 14:30:24] Step  5670 | loss 0.059768 | lr: 7.97e-07 | 0.114s per step\n",
      "[INFO][06/15/2020 14:30:48] Step  5880 | loss 0.055992 | lr: 3.66e-07 | 0.113s per step\n",
      "[INFO][06/15/2020 14:31:11] Step  6090 | loss 0.061142 | lr: 9.87e-08 | 0.112s per step\n",
      "[INFO][06/15/2020 14:31:35] Step  6300 | loss 0.058047 | lr: 4.96e-10 | 0.114s per step\n",
      "[INFO][06/15/2020 14:31:37] Metrics at step 6312:\n",
      "[INFO][06/15/2020 14:31:37] loss: 0.291706\n",
      "[INFO][06/15/2020 14:31:37] accuracy: 91.97%\n",
      "[INFO][06/15/2020 14:31:37] Training finished. Best step(s):\n",
      "[INFO][06/15/2020 14:31:37] loss: 0.237897 @ step 4208\n",
      "[INFO][06/15/2020 14:31:37] accuracy: 92.20% @ step 5260\n"
     ]
    }
   ],
   "source": [
    "print(total_steps)\n",
    "bot.train(\n",
    "    total_steps=total_steps,\n",
    "    checkpoint_interval=len(train_loader) // 2\n",
    ")\n",
    "bot.load_model(checkpoints.best_performers[0][1])\n",
    "checkpoints.remove_checkpoints(keep=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIR = CACHE_DIR / \"sst2_bert_uncased\"\n",
    "TARGET_DIR.mkdir(exist_ok=True)\n",
    "bot.model.save_pretrained(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': (0.28851168882956196, '0.288512'),\n",
       " 'accuracy': (-0.9220183486238532, '92.20%')}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.eval(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': (0.19422287623816675, '0.194223'),\n",
       " 'accuracy': (-0.9380733944954128, '93.81%')}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.eval(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('rapids': conda)",
   "language": "python",
   "name": "python37664bitrapidsconda05ea436670824d2ebed703c8c53b011e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
